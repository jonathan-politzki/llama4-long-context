version: '3.8'

services:
  llama-test:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      # Mount these directories to persist data between runs
      - ./comparison_results:/app/comparison_results
      - ./hf_cache:/app/.cache
    environment:
      # Set environment variables here
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu] 